{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5dba66e72ee0486e027e16e76828924844942279cc40e07d7f1b688e0ff7b112"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\zenga\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    filesdict = {}\n",
    "    for f in os.listdir(directory):\n",
    "        if f.endswith(\".txt\"):\n",
    "            #fpath = os.path.join(directory, filename)\n",
    "            with open(os.path.join(directory, filename)) as txtf:\n",
    "                contents = txtf.read()\n",
    "                filesdict[filename] = contents\n",
    "    return filesdict\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "os.listdir()\n",
    "f = {}\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_string = file.read()\n",
    "                file_dict[filename] = file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\n['ans', 'ad', 'asdds', 'bombsz', 'xd']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "tokens = nltk.word_tokenize(\"ans,ad, ASDDS is the BOmbsZ XD.\".lower())\n",
    "puncts_re =  \"[\" + string.punctuation + \"]\"\n",
    "print(puncts_re)\n",
    "print([word  for word in tokens if not re.search(puncts_re, word) and word not in stopwords ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: 1, 2: 2, 3: 2, 4: 2, 5: 1, 6: 1, 8: 1}\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,3,2,4,5,6,8]\n",
    "adict = {}\n",
    "for i in a:\n",
    "    if i in adict:\n",
    "        adict[i]+=1\n",
    "    else:\n",
    "        adict[i] = 1\n",
    "print(adict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "files = {\"a.txt\":[\"abc\",\"cbd\",\"csdf\"],\n",
    "        \"b.txt\": [\"abc\",\"zxcd\",\"asd\"]}\n",
    "idfs = {\"abc\": 1,\n",
    "        \"cbd\": 2,\n",
    "        \"asd\":3}\n",
    "print(len(idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a.txt', 'b.txt']\n"
     ]
    }
   ],
   "source": [
    "s = sum((idfs[word] for word in query if word in idfs))\n",
    "print(s)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "query = {\"abc\",\"cbd\",\"asd\"}\n",
    "filescores = {}\n",
    "n=2\n",
    "for f in files:\n",
    "    sum_tf=0\n",
    "    wordcount = collections.Counter(files[f])\n",
    "    for word in query:\n",
    "        if word in wordcount:\n",
    "            tf = wordcount[word] #number of times word appears in file\n",
    "            tfidf = tf*idfs[word]\n",
    "            sum_tf += tfidf\n",
    "    filescores[f] = sum_tf\n",
    "    sortedscores = sorted(filescores.keys(), key = lambda x:filescores[x])\n",
    "print(sortedscores[len(sortedscores)-n:]) \n",
    "        #if word in files[f]: #if word in document"
   ]
  }
 ]
}